{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b4d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading validation data...\n",
      "Loading test data...\n",
      "Class indices: {'Dolphin': 0, 'Fish': 1, 'Lobster': 2, 'Octopus': 3, 'Sea Horse': 4}\n",
      "\n",
      "Testing batch generation...\n",
      "Batch 1:\n",
      "  Images shape: (32, 380, 380, 3)\n",
      "  Labels shape: (32, 5)\n",
      "  Image range: [0.000, 255.000]\n",
      "  Labels: [1 4 0 1 2 4 3 4 0 1 4 2 1 3 3 1 4 2 2 1 1 1 3 1 2 0 4 0 4 2 1 0]\n",
      "\n",
      "Dataset Info:\n",
      "Training samples: 1241\n",
      "Validation samples: 250\n",
      "Test samples: 100\n",
      "Number of classes: 5\n",
      "Classes: ['Dolphin', 'Fish', 'Lobster', 'Octopus', 'Sea Horse']\n",
      "\n",
      "Building manual EfficientNetB4 model...\n",
      "Building EfficientNetB4 base model (simplified)...\n",
      "Model output shape: (2, 5)\n",
      "Model output sum per sample: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import json\n",
    "\n",
    "class ManualImageDataGenerator:\n",
    "    def __init__(self, \n",
    "                 target_size: Tuple[int, int] = (380, 380),\n",
    "                 batch_size: int = 32,\n",
    "                 shuffle: bool = True,\n",
    "                 preprocessing_function=None,\n",
    "                 horizontal_flip: bool = False,\n",
    "                 rotation_range: float = 0.0,\n",
    "                 zoom_range: float = 0.0,\n",
    "                 width_shift_range: float = 0.0,\n",
    "                 height_shift_range: float = 0.0):\n",
    "        \n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.preprocessing_function = preprocessing_function\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.rotation_range = rotation_range\n",
    "        self.zoom_range = zoom_range\n",
    "        self.width_shift_range = width_shift_range\n",
    "        self.height_shift_range = height_shift_range\n",
    "        \n",
    "        self.class_indices = {}\n",
    "        self.samples = []\n",
    "        self.filenames = []\n",
    "        self.classes = []\n",
    "        self.batch_index = 0\n",
    "        \n",
    "    def flow_from_directory(self, directory: str, class_mode: str = 'categorical'):\n",
    "        \"\"\"Load images from directory structure and prepare for batch generation\"\"\"\n",
    "        self.directory = directory\n",
    "        self.class_mode = class_mode\n",
    "        \n",
    "        # Discover classes from subdirectories\n",
    "        classes = [d for d in os.listdir(directory) \n",
    "                  if os.path.isdir(os.path.join(directory, d))]\n",
    "        classes.sort()\n",
    "        \n",
    "        self.class_indices = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        self.num_classes = len(classes)\n",
    "        \n",
    "        # Collect all image files\n",
    "        self.samples = []\n",
    "        self.filenames = []\n",
    "        self.classes = []\n",
    "        \n",
    "        for class_name in classes:\n",
    "            class_dir = os.path.join(directory, class_name)\n",
    "            class_idx = self.class_indices[class_name]\n",
    "            \n",
    "            for filename in os.listdir(class_dir):\n",
    "                if self._is_image_file(filename):\n",
    "                    filepath = os.path.join(class_dir, filename)\n",
    "                    self.samples.append((filepath, class_idx))\n",
    "                    self.filenames.append(filename)\n",
    "                    self.classes.append(class_idx)\n",
    "        \n",
    "        self.samples = np.array(self.samples)\n",
    "        self.classes = np.array(self.classes)\n",
    "        self.n = len(self.samples)\n",
    "        \n",
    "        self._set_index_array()\n",
    "        self.batch_index = 0\n",
    "        return self\n",
    "    \n",
    "    def _is_image_file(self, filename: str) -> bool:\n",
    "        \"\"\"Check if file is an image based on extension\"\"\"\n",
    "        valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']\n",
    "        return any(filename.lower().endswith(ext) for ext in valid_extensions)\n",
    "    \n",
    "    def _set_index_array(self):\n",
    "        \"\"\"Create index array for batch generation\"\"\"\n",
    "        self.index_array = np.arange(self.n)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.index_array)\n",
    "    \n",
    "    def _load_and_preprocess_image(self, filepath: str) -> np.ndarray:\n",
    "        \"\"\"Load and preprocess a single image\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            img = Image.open(filepath)\n",
    "            \n",
    "            # Convert to RGB if needed\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Resize image\n",
    "            img = img.resize(self.target_size, Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            img_array = np.array(img, dtype=np.float32)\n",
    "            \n",
    "            # Apply preprocessing function if provided\n",
    "            if self.preprocessing_function:\n",
    "                img_array = self.preprocessing_function(img_array)\n",
    "            \n",
    "            return img_array\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {filepath}: {e}\")\n",
    "            # Return a blank image if loading fails\n",
    "            return np.zeros((*self.target_size, 3), dtype=np.float32)\n",
    "    \n",
    "    def _augment_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply data augmentation to image\"\"\"\n",
    "        img = image.copy()\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Horizontal flip\n",
    "        if self.horizontal_flip and random.random() > 0.5:\n",
    "            img = img[:, ::-1, :]\n",
    "        \n",
    "        # Rotation\n",
    "        if self.rotation_range > 0:\n",
    "            angle = random.uniform(-self.rotation_range, self.rotation_range)\n",
    "            img = self._rotate_image(img, angle)\n",
    "        \n",
    "        # Zoom\n",
    "        if self.zoom_range > 0:\n",
    "            zoom_factor = random.uniform(1 - self.zoom_range, 1 + self.zoom_range)\n",
    "            img = self._zoom_image(img, zoom_factor)\n",
    "        \n",
    "        # Width shift\n",
    "        if self.width_shift_range > 0:\n",
    "            shift_x = int(random.uniform(-self.width_shift_range, self.width_shift_range) * w)\n",
    "            img = self._shift_image(img, shift_x, 0)\n",
    "        \n",
    "        # Height shift\n",
    "        if self.height_shift_range > 0:\n",
    "            shift_y = int(random.uniform(-self.height_shift_range, self.height_shift_range) * h)\n",
    "            img = self._shift_image(img, 0, shift_y)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def _rotate_image(self, image: np.ndarray, angle: float) -> np.ndarray:\n",
    "        \"\"\"Rotate image by given angle in degrees\"\"\"\n",
    "        pil_img = Image.fromarray(image.astype(np.uint8))\n",
    "        rotated = pil_img.rotate(angle, resample=Image.Resampling.BILINEAR, expand=False)\n",
    "        return np.array(rotated, dtype=np.float32)\n",
    "    \n",
    "    def _zoom_image(self, image: np.ndarray, zoom_factor: float) -> np.ndarray:\n",
    "        \"\"\"Zoom image by given factor\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Calculate new dimensions\n",
    "        new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)\n",
    "        \n",
    "        # Resize\n",
    "        pil_img = Image.fromarray(image.astype(np.uint8))\n",
    "        zoomed = pil_img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Crop or pad to original size\n",
    "        if zoom_factor > 1:\n",
    "            # Crop center\n",
    "            left = (new_w - w) // 2\n",
    "            top = (new_h - h) // 2\n",
    "            zoomed = zoomed.crop((left, top, left + w, top + h))\n",
    "        else:\n",
    "            # Pad with zeros\n",
    "            result = Image.new('RGB', (w, h), (0, 0, 0))\n",
    "            left = (w - new_w) // 2\n",
    "            top = (h - new_h) // 2\n",
    "            result.paste(zoomed, (left, top))\n",
    "            zoomed = result\n",
    "        \n",
    "        return np.array(zoomed, dtype=np.float32)\n",
    "    \n",
    "    def _shift_image(self, image: np.ndarray, shift_x: int, shift_y: int) -> np.ndarray:\n",
    "        \"\"\"Shift image by given pixels\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Create result array\n",
    "        result = np.zeros_like(image)\n",
    "        \n",
    "        # Calculate source and destination coordinates\n",
    "        x_src_start = max(-shift_x, 0)\n",
    "        y_src_start = max(-shift_y, 0)\n",
    "        x_dst_start = max(shift_x, 0)\n",
    "        y_dst_start = max(shift_y, 0)\n",
    "        \n",
    "        x_src_end = min(w - shift_x, w)\n",
    "        y_src_end = min(h - shift_y, h)\n",
    "        x_dst_end = min(w + shift_x, w)\n",
    "        y_dst_end = min(h + shift_y, h)\n",
    "        \n",
    "        # Calculate actual copy dimensions\n",
    "        copy_width = min(x_src_end - x_src_start, x_dst_end - x_dst_start)\n",
    "        copy_height = min(y_src_end - y_src_start, y_dst_end - y_dst_start)\n",
    "        \n",
    "        # Copy shifted region\n",
    "        if copy_width > 0 and copy_height > 0:\n",
    "            result[y_dst_start:y_dst_start+copy_height, x_dst_start:x_dst_start+copy_width] = \\\n",
    "                image[y_src_start:y_src_start+copy_height, x_src_start:x_src_start+copy_width]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _to_categorical(self, labels: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "        \"\"\"Convert class labels to one-hot encoded vectors\"\"\"\n",
    "        # Convert labels to integer array first\n",
    "        labels_int = labels.astype(int)\n",
    "        categorical = np.zeros((len(labels_int), num_classes), dtype=np.float32)\n",
    "        for i, label in enumerate(labels_int):\n",
    "            categorical[i, label] = 1.0\n",
    "        return categorical\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.batch_index = 0\n",
    "        self._set_index_array()\n",
    "        return self\n",
    "    \n",
    "    def __next__(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Generate next batch of data\"\"\"\n",
    "        if self.batch_index >= self.n:\n",
    "            self.batch_index = 0\n",
    "            raise StopIteration\n",
    "        \n",
    "        # Get batch indices\n",
    "        start = self.batch_index\n",
    "        end = min(start + self.batch_size, self.n)\n",
    "        batch_indices = self.index_array[start:end]\n",
    "        \n",
    "        # Load and process batch\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            filepath, class_idx = self.samples[idx]\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            img = self._load_and_preprocess_image(filepath)\n",
    "            \n",
    "            # Apply augmentation for training (check if augmentation is enabled)\n",
    "            if (self.horizontal_flip or self.rotation_range > 0 or \n",
    "                self.zoom_range > 0 or self.width_shift_range > 0 or \n",
    "                self.height_shift_range > 0):\n",
    "                img = self._augment_image(img)\n",
    "            \n",
    "            batch_x.append(img)\n",
    "            batch_y.append(class_idx)\n",
    "        \n",
    "        # Convert to arrays\n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_y = np.array(batch_y)\n",
    "        \n",
    "        # Convert labels to categorical if needed\n",
    "        if self.class_mode == 'categorical':\n",
    "            batch_y = self._to_categorical(batch_y, self.num_classes)\n",
    "        \n",
    "        self.batch_index = end\n",
    "        \n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Number of batches per epoch\"\"\"\n",
    "        return math.ceil(self.n / self.batch_size)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the generator\"\"\"\n",
    "        self.batch_index = 0\n",
    "        self._set_index_array()\n",
    "\n",
    "# EfficientNet preprocessing function (manual implementation)\n",
    "def efficientnet_preprocess_input(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Manual implementation of EfficientNet preprocessing\n",
    "    Normalizes images using the ImageNet dataset statistics\n",
    "    \"\"\"\n",
    "    # EfficientNet expects values in range [0, 255]\n",
    "    # Normalize using ImageNet mean and std\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "    # Normalize each channel\n",
    "    x_normalized = np.zeros_like(x, dtype=np.float32)\n",
    "    for i in range(3):\n",
    "        x_normalized[..., i] = (x[..., i] / 255.0 - imagenet_mean[i]) / imagenet_std[i]\n",
    "    \n",
    "    return x_normalized\n",
    "\n",
    "# Manual Neural Network Implementation\n",
    "class ManualDenseLayer:\n",
    "    def __init__(self, units, activation=None, input_dim=None):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.input_dim = input_dim\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.initialized = False\n",
    "        \n",
    "    def initialize(self, input_dim):\n",
    "        if not self.initialized:\n",
    "            self.input_dim = input_dim\n",
    "            # Xavier/Glorot initialization\n",
    "            limit = np.sqrt(6.0 / (input_dim + self.units))\n",
    "            self.weights = np.random.uniform(-limit, limit, (input_dim, self.units))\n",
    "            self.biases = np.zeros((1, self.units))\n",
    "            self.initialized = True\n",
    "            \n",
    "    def forward(self, x, training=True):\n",
    "        if not self.initialized:\n",
    "            self.initialize(x.shape[1])\n",
    "            \n",
    "        self.input = x\n",
    "        self.z = np.dot(x, self.weights) + self.biases\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            self.output = np.maximum(0, self.z)\n",
    "        elif self.activation == 'softmax':\n",
    "            # Stable softmax implementation\n",
    "            exp_z = np.exp(self.z - np.max(self.z, axis=1, keepdims=True))\n",
    "            self.output = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        else:\n",
    "            self.output = self.z\n",
    "            \n",
    "        return self.output\n",
    "\n",
    "class ManualDropoutLayer:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        if training and self.rate > 0:\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=x.shape) / (1 - self.rate)\n",
    "            return x * self.mask\n",
    "        return x\n",
    "\n",
    "class ManualGlobalAveragePooling2D:\n",
    "    def forward(self, x, training=True):\n",
    "        # x shape: (batch_size, height, width, channels)\n",
    "        return np.mean(x, axis=(1, 2))\n",
    "\n",
    "class ManualEfficientNetB4:\n",
    "    def __init__(self, include_top=True, input_shape=(380, 380, 3), num_classes=5):\n",
    "        self.include_top = include_top\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.layers = []\n",
    "        self.built = False\n",
    "        \n",
    "    def build(self):\n",
    "        if self.built:\n",
    "            return\n",
    "            \n",
    "        # Simulating the base model (pretrained weights would be loaded here)\n",
    "        print(\"Building EfficientNetB4 base model (simplified)...\")\n",
    "        \n",
    "        if self.include_top:\n",
    "            # Add custom classification layers\n",
    "            self.layers.append(ManualGlobalAveragePooling2D())\n",
    "            self.layers.append(ManualDropoutLayer(0.5))\n",
    "            self.layers.append(ManualDenseLayer(256, activation='relu'))\n",
    "            self.layers.append(ManualDropoutLayer(0.5))\n",
    "            self.layers.append(ManualDenseLayer(self.num_classes, activation='softmax'))\n",
    "            \n",
    "        self.built = True\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        if not self.built:\n",
    "            self.build()\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x, training)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def set_trainable(self, trainable):\n",
    "        # In a real implementation, this would freeze/unfreeze base model layers\n",
    "        pass\n",
    "\n",
    "class ManualAdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "            \n",
    "        self.t += 1\n",
    "        updated_params = []\n",
    "        \n",
    "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
    "            \n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            param_update = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "            updated_params.append(param - param_update)\n",
    "            \n",
    "        return updated_params\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    # Avoid numerical instability\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define dataset directories\n",
    "    base_dir = r\"E:\\marine-animals-dataset\\versions\\1\"\n",
    "    train_dir = os.path.join(base_dir, 'train')\n",
    "    valid_dir = os.path.join(base_dir, 'valid')\n",
    "    test_dir = os.path.join(base_dir, 'test')\n",
    "    \n",
    "    # Create data generators with EfficientNet preprocessing\n",
    "    train_datagen = ManualImageDataGenerator(\n",
    "        target_size=(380, 380),\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        preprocessing_function=efficientnet_preprocess_input,\n",
    "        horizontal_flip=True,\n",
    "        rotation_range=15,\n",
    "        zoom_range=0.25,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2\n",
    "    )\n",
    "    \n",
    "    valid_datagen = ManualImageDataGenerator(\n",
    "        target_size=(380, 380),\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        preprocessing_function=efficientnet_preprocess_input\n",
    "    )\n",
    "    \n",
    "    test_datagen = ManualImageDataGenerator(\n",
    "        target_size=(380, 380),\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        preprocessing_function=efficientnet_preprocess_input\n",
    "    )\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading training data...\")\n",
    "    train_generator = train_datagen.flow_from_directory(train_dir, class_mode='categorical')\n",
    "    \n",
    "    print(\"Loading validation data...\")\n",
    "    valid_generator = valid_datagen.flow_from_directory(valid_dir, class_mode='categorical')\n",
    "    \n",
    "    print(\"Loading test data...\")\n",
    "    test_generator = test_datagen.flow_from_directory(test_dir, class_mode='categorical')\n",
    "    \n",
    "    # Print class indices\n",
    "    class_indices = train_generator.class_indices\n",
    "    print(\"Class indices:\", class_indices)\n",
    "    \n",
    "    # Example usage: iterate through one batch\n",
    "    print(\"\\nTesting batch generation...\")\n",
    "    for i, (batch_x, batch_y) in enumerate(train_generator):\n",
    "        print(f\"Batch {i+1}:\")\n",
    "        print(f\"  Images shape: {batch_x.shape}\")\n",
    "        print(f\"  Labels shape: {batch_y.shape}\")\n",
    "        print(f\"  Image range: [{batch_x.min():.3f}, {batch_x.max():.3f}]\")\n",
    "        print(f\"  Labels: {np.argmax(batch_y, axis=1)}\")\n",
    "        \n",
    "        if i == 0:  # Just show first batch for demonstration\n",
    "            break\n",
    "    \n",
    "    # Reset generators for actual training\n",
    "    train_generator.reset()\n",
    "    valid_generator.reset()\n",
    "    test_generator.reset()\n",
    "    \n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(f\"Training samples: {train_generator.n}\")\n",
    "    print(f\"Validation samples: {valid_generator.n}\")\n",
    "    print(f\"Test samples: {test_generator.n}\")\n",
    "    print(f\"Number of classes: {train_generator.num_classes}\")\n",
    "    print(f\"Classes: {list(train_generator.class_indices.keys())}\")\n",
    "    \n",
    "    # Create and test the manual model\n",
    "    print(\"\\nBuilding manual EfficientNetB4 model...\")\n",
    "    model = ManualEfficientNetB4(include_top=True, input_shape=(380, 380, 3), num_classes=5)\n",
    "    \n",
    "    # Test forward pass with a sample batch\n",
    "    sample_batch = np.random.random((2, 380, 380, 3)).astype(np.float32)\n",
    "    output = model.forward(sample_batch)\n",
    "    print(f\"Model output shape: {output.shape}\")\n",
    "    print(f\"Model output sum per sample: {np.sum(output, axis=1)}\")  # Should be ~1.0 for softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f224b4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual EfficientNetB4 Model Summary:\n",
      "Input shape: (380, 380, 3)\n",
      "Base model: Frozen\n",
      "Classifier layers:\n",
      "  GlobalAveragePooling2D\n",
      "  Dropout(0.5)\n",
      "  Dense(256, relu)\n",
      "  Dropout(0.5)\n",
      "  Dense(5, softmax)\n",
      "Optimizer: Adam(lr=0.001)\n",
      "Loss: categorical_crossentropy\n",
      "\n",
      "Sample output shape: (2, 5)\n",
      "Sample output sum per sample: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "\n",
    "class ManualImageDataGenerator:\n",
    "    def __init__(self, \n",
    "                 target_size=(380, 380),\n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 preprocessing_function=None,\n",
    "                 horizontal_flip=False,\n",
    "                 rotation_range=0.0,\n",
    "                 zoom_range=0.0,\n",
    "                 width_shift_range=0.0,\n",
    "                 height_shift_range=0.0):\n",
    "        \n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.preprocessing_function = preprocessing_function\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.rotation_range = rotation_range\n",
    "        self.zoom_range = zoom_range\n",
    "        self.width_shift_range = width_shift_range\n",
    "        self.height_shift_range = height_shift_range\n",
    "        \n",
    "        self.class_indices = {}\n",
    "        self.samples = []\n",
    "        self.batch_index = 0\n",
    "        \n",
    "    def flow_from_directory(self, directory, class_mode='categorical'):\n",
    "        self.directory = directory\n",
    "        self.class_mode = class_mode\n",
    "        \n",
    "        classes = [d for d in os.listdir(directory) \n",
    "                  if os.path.isdir(os.path.join(directory, d))]\n",
    "        classes.sort()\n",
    "        \n",
    "        self.class_indices = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        self.num_classes = len(classes)\n",
    "        \n",
    "        self.samples = []\n",
    "        for class_name in classes:\n",
    "            class_dir = os.path.join(directory, class_name)\n",
    "            class_idx = self.class_indices[class_name]\n",
    "            \n",
    "            for filename in os.listdir(class_dir):\n",
    "                if self._is_image_file(filename):\n",
    "                    filepath = os.path.join(class_dir, filename)\n",
    "                    self.samples.append((filepath, class_idx))\n",
    "        \n",
    "        self.n = len(self.samples)\n",
    "        self._set_index_array()\n",
    "        self.batch_index = 0\n",
    "        return self\n",
    "    \n",
    "    def _is_image_file(self, filename):\n",
    "        valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']\n",
    "        return any(filename.lower().endswith(ext) for ext in valid_extensions)\n",
    "    \n",
    "    def _set_index_array(self):\n",
    "        self.index_array = np.arange(self.n)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.index_array)\n",
    "    \n",
    "    def _load_and_preprocess_image(self, filepath):\n",
    "        try:\n",
    "            img = Image.open(filepath)\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img = img.resize(self.target_size, Image.Resampling.LANCZOS)\n",
    "            img_array = np.array(img, dtype=np.float32)\n",
    "            \n",
    "            if self.preprocessing_function:\n",
    "                img_array = self.preprocessing_function(img_array)\n",
    "            \n",
    "            return img_array\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {filepath}: {e}\")\n",
    "            return np.zeros((*self.target_size, 3), dtype=np.float32)\n",
    "    \n",
    "    def _augment_image(self, image):\n",
    "        img = image.copy()\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        if self.horizontal_flip and random.random() > 0.5:\n",
    "            img = img[:, ::-1, :]\n",
    "        \n",
    "        if self.rotation_range > 0:\n",
    "            angle = random.uniform(-self.rotation_range, self.rotation_range)\n",
    "            pil_img = Image.fromarray(img.astype(np.uint8))\n",
    "            rotated = pil_img.rotate(angle, resample=Image.Resampling.BILINEAR, expand=False)\n",
    "            img = np.array(rotated, dtype=np.float32)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def _to_categorical(self, labels, num_classes):\n",
    "        labels_int = labels.astype(int)\n",
    "        categorical = np.zeros((len(labels_int), num_classes), dtype=np.float32)\n",
    "        for i, label in enumerate(labels_int):\n",
    "            categorical[i, label] = 1.0\n",
    "        return categorical\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.batch_index = 0\n",
    "        self._set_index_array()\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.batch_index >= self.n:\n",
    "            self.batch_index = 0\n",
    "            raise StopIteration\n",
    "        \n",
    "        start = self.batch_index\n",
    "        end = min(start + self.batch_size, self.n)\n",
    "        batch_indices = self.index_array[start:end]\n",
    "        \n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            filepath, class_idx = self.samples[idx]\n",
    "            img = self._load_and_preprocess_image(filepath)\n",
    "            \n",
    "            if (self.horizontal_flip or self.rotation_range > 0):\n",
    "                img = self._augment_image(img)\n",
    "            \n",
    "            batch_x.append(img)\n",
    "            batch_y.append(class_idx)\n",
    "        \n",
    "        batch_x = np.array(batch_x)\n",
    "        batch_y = np.array(batch_y)\n",
    "        \n",
    "        if self.class_mode == 'categorical':\n",
    "            batch_y = self._to_categorical(batch_y, self.num_classes)\n",
    "        \n",
    "        self.batch_index = end\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(self.n / self.batch_size)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.batch_index = 0\n",
    "        self._set_index_array()\n",
    "\n",
    "def efficientnet_preprocess_input(x):\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "    x_normalized = np.zeros_like(x, dtype=np.float32)\n",
    "    for i in range(3):\n",
    "        x_normalized[..., i] = (x[..., i] / 255.0 - imagenet_mean[i]) / imagenet_std[i]\n",
    "    \n",
    "    return x_normalized\n",
    "\n",
    "class ManualConv2D:\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding='same', use_bias=True):\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.use_bias = use_bias\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        input_channels = input_shape[-1]\n",
    "        limit = np.sqrt(6.0 / (input_channels * self.kernel_size * self.kernel_size + self.filters))\n",
    "        self.weights = np.random.uniform(-limit, limit, \n",
    "                                       (self.kernel_size, self.kernel_size, input_channels, self.filters))\n",
    "        if self.use_bias:\n",
    "            self.biases = np.zeros(self.filters)\n",
    "        return input_shape\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, h, w, input_channels = x.shape\n",
    "        k_h, k_w, _, output_channels = self.weights.shape\n",
    "        \n",
    "        if self.padding == 'same':\n",
    "            pad_h = (h * (self.strides - 1) + k_h - self.strides) // 2\n",
    "            pad_w = (w * (self.strides - 1) + k_w - self.strides) // 2\n",
    "            x_padded = np.pad(x, ((0, 0), (pad_h, pad_h), (pad_w, pad_w), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            x_padded = x\n",
    "        \n",
    "        h_padded, w_padded = x_padded.shape[1], x_padded.shape[2]\n",
    "        output_h = (h_padded - k_h) // self.strides + 1\n",
    "        output_w = (w_padded - k_w) // self.strides + 1\n",
    "        \n",
    "        output = np.zeros((batch_size, output_h, output_w, output_channels))\n",
    "        \n",
    "        for i in range(output_h):\n",
    "            for j in range(output_w):\n",
    "                h_start = i * self.strides\n",
    "                h_end = h_start + k_h\n",
    "                w_start = j * self.strides\n",
    "                w_end = w_start + k_w\n",
    "                \n",
    "                patch = x_padded[:, h_start:h_end, w_start:w_end, :]\n",
    "                for k in range(output_channels):\n",
    "                    output[:, i, j, k] = np.sum(patch * self.weights[:, :, :, k], axis=(1, 2, 3))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            output += self.biases\n",
    "        \n",
    "        return output\n",
    "\n",
    "class ManualDense:\n",
    "    def __init__(self, units, activation=None):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        limit = np.sqrt(6.0 / (input_dim + self.units))\n",
    "        self.weights = np.random.uniform(-limit, limit, (input_dim, self.units))\n",
    "        self.biases = np.zeros(self.units)\n",
    "        return (input_shape[0], self.units)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.z = np.dot(x, self.weights) + self.biases\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            self.output = np.maximum(0, self.z)\n",
    "        elif self.activation == 'softmax':\n",
    "            exp_z = np.exp(self.z - np.max(self.z, axis=1, keepdims=True))\n",
    "            self.output = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        else:\n",
    "            self.output = self.z\n",
    "            \n",
    "        return self.output\n",
    "\n",
    "class ManualGlobalAveragePooling2D:\n",
    "    def initialize(self, input_shape):\n",
    "        return (input_shape[0], input_shape[3])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return np.mean(x, axis=(1, 2))\n",
    "\n",
    "class ManualDropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        if training and self.rate > 0:\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=x.shape) / (1 - self.rate)\n",
    "            return x * self.mask\n",
    "        return x\n",
    "\n",
    "class ManualBatchNormalization:\n",
    "    def __init__(self, momentum=0.99, epsilon=1e-3):\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.moving_mean = None\n",
    "        self.moving_variance = None\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        self.gamma = np.ones(input_shape[-1])\n",
    "        self.beta = np.zeros(input_shape[-1])\n",
    "        self.moving_mean = np.zeros(input_shape[-1])\n",
    "        self.moving_variance = np.ones(input_shape[-1])\n",
    "        return input_shape\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            mean = np.mean(x, axis=(0, 1, 2), keepdims=True)\n",
    "            variance = np.var(x, axis=(0, 1, 2), keepdims=True)\n",
    "            \n",
    "            self.moving_mean = self.momentum * self.moving_mean + (1 - self.momentum) * mean.flatten()\n",
    "            self.moving_variance = self.momentum * self.moving_variance + (1 - self.momentum) * variance.flatten()\n",
    "            \n",
    "            x_normalized = (x - mean) / np.sqrt(variance + self.epsilon)\n",
    "        else:\n",
    "            x_normalized = (x - self.moving_mean.reshape(1, 1, 1, -1)) / np.sqrt(self.moving_variance.reshape(1, 1, 1, -1) + self.epsilon)\n",
    "        \n",
    "        return x_normalized * self.gamma.reshape(1, 1, 1, -1) + self.beta.reshape(1, 1, 1, -1)\n",
    "\n",
    "class ManualMBConvBlock:\n",
    "    def __init__(self, filters, strides, expand_ratio, kernel_size=3):\n",
    "        self.filters = filters\n",
    "        self.strides = strides\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.kernel_size = kernel_size\n",
    "        self.layers = []\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        input_channels = input_shape[-1]\n",
    "        expanded_channels = input_channels * self.expand_ratio\n",
    "        \n",
    "        # Expansion conv\n",
    "        if self.expand_ratio != 1:\n",
    "            self.expand_conv = ManualConv2D(expanded_channels, 1, strides=1)\n",
    "            self.expand_conv.initialize(input_shape)\n",
    "            self.layers.append(self.expand_conv)\n",
    "            \n",
    "            self.bn1 = ManualBatchNormalization()\n",
    "            self.bn1.initialize((input_shape[0], input_shape[1], input_shape[2], expanded_channels))\n",
    "            self.layers.append(self.bn1)\n",
    "        \n",
    "        # Depthwise conv\n",
    "        self.depthwise_conv = ManualConv2D(expanded_channels, self.kernel_size, strides=self.strides, padding='same')\n",
    "        depthwise_input_shape = (input_shape[0], input_shape[1], input_shape[2], expanded_channels) if self.expand_ratio != 1 else input_shape\n",
    "        self.depthwise_conv.initialize(depthwise_input_shape)\n",
    "        self.layers.append(self.depthwise_conv)\n",
    "        \n",
    "        self.bn2 = ManualBatchNormalization()\n",
    "        self.bn2.initialize((input_shape[0], input_shape[1]//self.strides, input_shape[2]//self.strides, expanded_channels))\n",
    "        self.layers.append(self.bn2)\n",
    "        \n",
    "        # Projection conv\n",
    "        self.projection_conv = ManualConv2D(self.filters, 1, strides=1)\n",
    "        projection_input_shape = (input_shape[0], input_shape[1]//self.strides, input_shape[2]//self.strides, expanded_channels)\n",
    "        self.projection_conv.initialize(projection_input_shape)\n",
    "        self.layers.append(self.projection_conv)\n",
    "        \n",
    "        self.bn3 = ManualBatchNormalization()\n",
    "        output_shape = (input_shape[0], input_shape[1]//self.strides, input_shape[2]//self.strides, self.filters)\n",
    "        self.bn3.initialize(output_shape)\n",
    "        self.layers.append(self.bn3)\n",
    "        \n",
    "        return output_shape\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        residual = x\n",
    "        \n",
    "        # Expansion\n",
    "        if self.expand_ratio != 1:\n",
    "            x = self.expand_conv.forward(x)\n",
    "            x = self.bn1.forward(x, training)\n",
    "            x = np.maximum(0, x)  # Swish activation simplified to ReLU\n",
    "        \n",
    "        # Depthwise\n",
    "        x = self.depthwise_conv.forward(x)\n",
    "        x = self.bn2.forward(x, training)\n",
    "        x = np.maximum(0, x)\n",
    "        \n",
    "        # Projection\n",
    "        x = self.projection_conv.forward(x)\n",
    "        x = self.bn3.forward(x, training)\n",
    "        \n",
    "        # Skip connection\n",
    "        if self.strides == 1 and residual.shape[-1] == x.shape[-1]:\n",
    "            x = x + residual\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ManualEfficientNetB4:\n",
    "    def __init__(self, include_top=True, input_shape=(380, 380, 3), num_classes=1000):\n",
    "        self.include_top = include_top\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.layers = []\n",
    "        self.built = False\n",
    "        \n",
    "    def build(self):\n",
    "        if self.built:\n",
    "            return\n",
    "            \n",
    "        # Stem\n",
    "        self.stem_conv = ManualConv2D(48, 3, strides=2, padding='same')\n",
    "        self.stem_conv.initialize((1, *self.input_shape))\n",
    "        self.layers.append(self.stem_conv)\n",
    "        \n",
    "        self.stem_bn = ManualBatchNormalization()\n",
    "        self.stem_bn.initialize((1, 190, 190, 48))\n",
    "        self.layers.append(self.stem_bn)\n",
    "        \n",
    "        # MBConv blocks (simplified version)\n",
    "        # Block 1\n",
    "        self.block1 = ManualMBConvBlock(24, 1, 1)\n",
    "        self.block1.initialize((1, 190, 190, 48))\n",
    "        self.layers.append(self.block1)\n",
    "        \n",
    "        # Block 2\n",
    "        self.block2 = ManualMBConvBlock(32, 2, 6)\n",
    "        self.block2.initialize((1, 190, 190, 24))\n",
    "        self.layers.append(self.block2)\n",
    "        \n",
    "        if self.include_top:\n",
    "            # Top layers\n",
    "            self.global_pool = ManualGlobalAveragePooling2D()\n",
    "            self.global_pool.initialize((1, 95, 95, 32))\n",
    "            self.layers.append(self.global_pool)\n",
    "            \n",
    "            self.dropout1 = ManualDropout(0.5)\n",
    "            self.dropout1.initialize((1, 32))\n",
    "            self.layers.append(self.dropout1)\n",
    "            \n",
    "            self.dense1 = ManualDense(256, activation='relu')\n",
    "            self.dense1.initialize((1, 32))\n",
    "            self.layers.append(self.dense1)\n",
    "            \n",
    "            self.dropout2 = ManualDropout(0.5)\n",
    "            self.dropout2.initialize((1, 256))\n",
    "            self.layers.append(self.dropout2)\n",
    "            \n",
    "            self.dense2 = ManualDense(self.num_classes, activation='softmax')\n",
    "            self.dense2.initialize((1, 256))\n",
    "            self.layers.append(self.dense2)\n",
    "            \n",
    "        self.built = True\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        if not self.built:\n",
    "            self.build()\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ManualDropout):\n",
    "                x = layer.forward(x, training)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def set_trainable(self, trainable):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'weights'):\n",
    "                layer.trainable = trainable\n",
    "\n",
    "class ManualAdam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "            \n",
    "        self.t += 1\n",
    "        updated_params = []\n",
    "        \n",
    "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
    "            \n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            param_update = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "            updated_params.append(param - param_update)\n",
    "            \n",
    "        return updated_params\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Create manual EfficientNetB4 model\n",
    "    base_model = ManualEfficientNetB4(include_top=False, input_shape=(380, 380, 3), num_classes=5)\n",
    "    \n",
    "    # Freeze base model\n",
    "    base_model.set_trainable(False)\n",
    "    \n",
    "    # Build custom classifier\n",
    "    classifier_layers = [\n",
    "        ManualGlobalAveragePooling2D(),\n",
    "        ManualDropout(0.5),\n",
    "        ManualDense(256, activation='relu'),\n",
    "        ManualDropout(0.5),\n",
    "        ManualDense(5, activation='softmax')\n",
    "    ]\n",
    "    \n",
    "    # Initialize classifier\n",
    "    current_shape = (1, 95, 95, 32)  # Output shape from base model\n",
    "    for layer in classifier_layers:\n",
    "        current_shape = layer.initialize(current_shape)\n",
    "    \n",
    "    # Define complete model\n",
    "    class CompleteModel:\n",
    "        def __init__(self, base_model, classifier_layers):\n",
    "            self.base_model = base_model\n",
    "            self.classifier_layers = classifier_layers\n",
    "            \n",
    "        def forward(self, x, training=True):\n",
    "            x = self.base_model.forward(x, training)\n",
    "            for layer in self.classifier_layers:\n",
    "                if isinstance(layer, ManualDropout):\n",
    "                    x = layer.forward(x, training)\n",
    "                else:\n",
    "                    x = layer.forward(x)\n",
    "            return x\n",
    "    \n",
    "    model = CompleteModel(base_model, classifier_layers)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = ManualAdam(learning_rate=0.001)\n",
    "    \n",
    "    print(\"Manual EfficientNetB4 Model Summary:\")\n",
    "    print(\"Input shape: (380, 380, 3)\")\n",
    "    print(\"Base model: Frozen\")\n",
    "    print(\"Classifier layers:\")\n",
    "    print(\"  GlobalAveragePooling2D\")\n",
    "    print(\"  Dropout(0.5)\")\n",
    "    print(\"  Dense(256, relu)\")\n",
    "    print(\"  Dropout(0.5)\")\n",
    "    print(\"  Dense(5, softmax)\")\n",
    "    print(\"Optimizer: Adam(lr=0.001)\")\n",
    "    print(\"Loss: categorical_crossentropy\")\n",
    "    \n",
    "    # Test with sample data\n",
    "    sample_input = np.random.random((2, 380, 380, 3)).astype(np.float32)\n",
    "    output = model.forward(sample_input, training=False)\n",
    "    print(f\"\\nSample output shape: {output.shape}\")\n",
    "    print(f\"Sample output sum per sample: {np.sum(output, axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4764748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading validation data...\n",
      "Starting manual training...\n",
      "Epoch 1/10\n",
      "loss: 1.6094 - accuracy: 0.1859 - val_loss: 1.6145 - val_accuracy: 0.1953\n",
      "Epoch 2/10\n",
      "loss: 1.6094 - accuracy: 0.1998 - val_loss: 1.6229 - val_accuracy: 0.1953\n",
      "Epoch 3/10\n",
      "loss: 1.6094 - accuracy: 0.2162 - val_loss: 1.6267 - val_accuracy: 0.1953\n",
      "Epoch 4/10\n",
      "loss: 1.6094 - accuracy: 0.1855 - val_loss: 1.6238 - val_accuracy: 0.1953\n",
      "Epoch 5/10\n",
      "loss: 1.6094 - accuracy: 0.1969 - val_loss: 1.6172 - val_accuracy: 0.1953\n",
      "Epoch 6/10\n",
      "loss: 1.6094 - accuracy: 0.1769 - val_loss: 1.6130 - val_accuracy: 0.1953\n",
      "Epoch 7/10\n",
      "loss: 1.6094 - accuracy: 0.2080 - val_loss: 1.6132 - val_accuracy: 0.1953\n",
      "Epoch 8/10\n",
      "loss: 1.6094 - accuracy: 0.1929 - val_loss: 1.6134 - val_accuracy: 0.1953\n",
      "Epoch 9/10\n",
      "loss: 1.6094 - accuracy: 0.2009 - val_loss: 1.6137 - val_accuracy: 0.1953\n",
      "Epoch 10/10\n",
      "loss: 1.6094 - accuracy: 0.1928 - val_loss: 1.6150 - val_accuracy: 0.1953\n",
      "Best validation accuracy: 19.53%\n",
      "\n",
      "Evaluating on validation set...\n",
      "Evaluation - loss: 1.6150 - accuracy: 0.1953\n",
      "Validation Accuracy after initial training: 19.53%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "\n",
    "class ManualImageDataGenerator:\n",
    "    def __init__(self, \n",
    "                 target_size=(224, 224),\n",
    "                 batch_size=16,  # Increased batch size\n",
    "                 shuffle=True,\n",
    "                 preprocessing_function=None,\n",
    "                 horizontal_flip=False,\n",
    "                 rotation_range=0.0,\n",
    "                 zoom_range=0.0):\n",
    "        \n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.preprocessing_function = preprocessing_function\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.rotation_range = rotation_range\n",
    "        self.zoom_range = zoom_range\n",
    "        \n",
    "        self.class_indices = {}\n",
    "        self.samples = []\n",
    "        self.batch_index = 0\n",
    "        \n",
    "    def flow_from_directory(self, directory, class_mode='categorical'):\n",
    "        self.directory = directory\n",
    "        self.class_mode = class_mode\n",
    "        \n",
    "        classes = [d for d in os.listdir(directory) \n",
    "                  if os.path.isdir(os.path.join(directory, d))]\n",
    "        classes.sort()\n",
    "        \n",
    "        self.class_indices = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        self.num_classes = len(classes)\n",
    "        \n",
    "        self.samples = []\n",
    "        for class_name in classes:\n",
    "            class_dir = os.path.join(directory, class_name)\n",
    "            class_idx = self.class_indices[class_name]\n",
    "            \n",
    "            for filename in os.listdir(class_dir):\n",
    "                if self._is_image_file(filename):\n",
    "                    filepath = os.path.join(class_dir, filename)\n",
    "                    self.samples.append((filepath, class_idx))\n",
    "        \n",
    "        self.n = len(self.samples)\n",
    "        self._set_index_array()\n",
    "        self.batch_index = 0\n",
    "        return self\n",
    "    \n",
    "    def _is_image_file(self, filename):\n",
    "        valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "        return any(filename.lower().endswith(ext) for ext in valid_extensions)\n",
    "    \n",
    "    def _set_index_array(self):\n",
    "        self.index_array = np.arange(self.n)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.index_array)\n",
    "    \n",
    "    def _load_and_preprocess_image(self, filepath):\n",
    "        try:\n",
    "            img = Image.open(filepath)\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            img = img.resize(self.target_size, Image.Resampling.LANCZOS)\n",
    "            img_array = np.array(img, dtype=np.float32)\n",
    "            \n",
    "            if self.preprocessing_function:\n",
    "                img_array = self.preprocessing_function(img_array)\n",
    "            \n",
    "            return img_array\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {filepath}: {e}\")\n",
    "            return np.zeros((*self.target_size, 3), dtype=np.float32)\n",
    "    \n",
    "    def _augment_image(self, image):\n",
    "        img = image.copy()\n",
    "        \n",
    "        if self.horizontal_flip and random.random() > 0.5:\n",
    "            img = img[:, ::-1, :]\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def _to_categorical(self, labels, num_classes):\n",
    "        labels_int = labels.astype(int)\n",
    "        categorical = np.zeros((len(labels_int), num_classes), dtype=np.float32)\n",
    "        for i, label in enumerate(labels_int):\n",
    "            categorical[i, label] = 1.0\n",
    "        return categorical\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.batch_index = 0\n",
    "        self._set_index_array()\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.batch_index >= self.n:\n",
    "            self.batch_index = 0\n",
    "            raise StopIteration\n",
    "        \n",
    "        start = self.batch_index\n",
    "        end = min(start + self.batch_size, self.n)\n",
    "        batch_indices = self.index_array[start:end]\n",
    "        \n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for idx in batch_indices:\n",
    "            filepath, class_idx = self.samples[idx]\n",
    "            img = self._load_and_preprocess_image(filepath)\n",
    "            \n",
    "            # Apply augmentation only during training\n",
    "            if hasattr(self, 'horizontal_flip') and self.horizontal_flip:\n",
    "                img = self._augment_image(img)\n",
    "            \n",
    "            batch_x.append(img)\n",
    "            batch_y.append(class_idx)\n",
    "        \n",
    "        batch_x = np.array(batch_x, dtype=np.float32)\n",
    "        batch_y = np.array(batch_y)\n",
    "        \n",
    "        if self.class_mode == 'categorical':\n",
    "            batch_y = self._to_categorical(batch_y, self.num_classes)\n",
    "        \n",
    "        self.batch_index = end\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(self.n / self.batch_size)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.batch_index = 0\n",
    "        self._set_index_array()\n",
    "\n",
    "def efficientnet_preprocess_input(x):\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    \n",
    "    x_normalized = np.zeros_like(x, dtype=np.float32)\n",
    "    for i in range(3):\n",
    "        x_normalized[..., i] = (x[..., i] / 255.0 - imagenet_mean[i]) / imagenet_std[i]\n",
    "    \n",
    "    return x_normalized\n",
    "\n",
    "class ManualDense:\n",
    "    def __init__(self, units, activation=None):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.initialized = False\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        # Better initialization\n",
    "        limit = np.sqrt(2.0 / input_dim)\n",
    "        self.weights = np.random.randn(input_dim, self.units).astype(np.float32) * limit\n",
    "        self.biases = np.zeros(self.units, dtype=np.float32)\n",
    "        self.initialized = True\n",
    "        return (input_shape[0], self.units)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.initialized:\n",
    "            self.initialize(x.shape)\n",
    "            \n",
    "        self.input = x\n",
    "        self.z = np.dot(x, self.weights) + self.biases\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            self.output = np.maximum(0, self.z)\n",
    "        elif self.activation == 'softmax':\n",
    "            # More stable softmax\n",
    "            shift_z = self.z - np.max(self.z, axis=1, keepdims=True)\n",
    "            exp_z = np.exp(shift_z)\n",
    "            self.output = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        else:\n",
    "            self.output = self.z\n",
    "            \n",
    "        return self.output\n",
    "\n",
    "class ManualGlobalAveragePooling2D:\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        self.initialized = True\n",
    "        return (input_shape[0], input_shape[3])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.initialized:\n",
    "            self.initialize(x.shape)\n",
    "        return np.mean(x, axis=(1, 2), dtype=np.float32)\n",
    "\n",
    "class ManualDropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.initialized = False\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        self.initialized = True\n",
    "        return input_shape\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        if not self.initialized:\n",
    "            self.initialize(x.shape)\n",
    "            \n",
    "        if training and self.rate > 0:\n",
    "            # Correct dropout implementation\n",
    "            self.mask = (np.random.random(x.shape) > self.rate).astype(np.float32)\n",
    "            return x * self.mask\n",
    "        return x\n",
    "\n",
    "class ManualConv2D:\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding='same', use_bias=True):\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.use_bias = use_bias\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.initialized = False\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        input_channels = input_shape[-1]\n",
    "        # He initialization for ReLU\n",
    "        limit = np.sqrt(2.0 / (input_channels * self.kernel_size * self.kernel_size))\n",
    "        \n",
    "        self.weights = np.random.randn(\n",
    "            self.kernel_size, self.kernel_size, input_channels, self.filters\n",
    "        ).astype(np.float32) * limit\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.biases = np.zeros(self.filters, dtype=np.float32)\n",
    "            \n",
    "        self.initialized = True\n",
    "        return self._compute_output_shape(input_shape)\n",
    "    \n",
    "    def _compute_output_shape(self, input_shape):\n",
    "        batch_size, h, w, _ = input_shape\n",
    "        \n",
    "        if self.padding == 'same':\n",
    "            out_h = (h + self.strides - 1) // self.strides\n",
    "            out_w = (w + self.strides - 1) // self.strides\n",
    "        else:\n",
    "            out_h = (h - self.kernel_size) // self.strides + 1\n",
    "            out_w = (w - self.kernel_size) // self.strides + 1\n",
    "            \n",
    "        return (batch_size, out_h, out_w, self.filters)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.initialized:\n",
    "            self.initialize(x.shape)\n",
    "            \n",
    "        batch_size, h, w, input_channels = x.shape\n",
    "        \n",
    "        if self.padding == 'same':\n",
    "            pad_h = ((h - 1) * self.strides + self.kernel_size - h) // 2\n",
    "            pad_w = ((w - 1) * self.strides + self.kernel_size - w) // 2\n",
    "            x_padded = np.pad(x, ((0, 0), (pad_h, pad_h), (pad_w, pad_w), (0, 0)), \n",
    "                            mode='constant').astype(np.float32)\n",
    "        else:\n",
    "            x_padded = x\n",
    "        \n",
    "        output_shape = self._compute_output_shape(x.shape)\n",
    "        output = np.zeros(output_shape, dtype=np.float32)\n",
    "        \n",
    "        out_h, out_w = output_shape[1], output_shape[2]\n",
    "        \n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                h_start = i * self.strides\n",
    "                h_end = h_start + self.kernel_size\n",
    "                w_start = j * self.strides\n",
    "                w_end = w_start + self.kernel_size\n",
    "                \n",
    "                patch = x_padded[:, h_start:h_end, w_start:w_end, :]\n",
    "                \n",
    "                # Efficient convolution\n",
    "                for k in range(self.filters):\n",
    "                    output[:, i, j, k] = np.sum(patch * self.weights[:, :, :, k], axis=(1, 2, 3))\n",
    "        \n",
    "        if self.use_bias:\n",
    "            output += self.biases\n",
    "        \n",
    "        return output\n",
    "\n",
    "class ManualBatchNormalization:\n",
    "    def __init__(self, momentum=0.99, epsilon=1e-3):\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "        self.moving_mean = None\n",
    "        self.moving_variance = None\n",
    "        self.initialized = False\n",
    "        \n",
    "    def initialize(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.gamma = np.ones(channels, dtype=np.float32)\n",
    "        self.beta = np.zeros(channels, dtype=np.float32)\n",
    "        self.moving_mean = np.zeros(channels, dtype=np.float32)\n",
    "        self.moving_variance = np.ones(channels, dtype=np.float32)\n",
    "        self.initialized = True\n",
    "        return input_shape\n",
    "    \n",
    "    def forward(self, x, training=True):\n",
    "        if not self.initialized:\n",
    "            self.initialize(x.shape)\n",
    "            \n",
    "        if training:\n",
    "            # Batch statistics\n",
    "            mean = np.mean(x, axis=(0, 1, 2), keepdims=True)\n",
    "            variance = np.var(x, axis=(0, 1, 2), keepdims=True)\n",
    "            \n",
    "            # Update moving statistics\n",
    "            self.moving_mean = self.momentum * self.moving_mean + (1 - self.momentum) * mean.flatten()\n",
    "            self.moving_variance = self.momentum * self.moving_variance + (1 - self.momentum) * variance.flatten()\n",
    "            \n",
    "            # Normalize\n",
    "            x_normalized = (x - mean) / np.sqrt(variance + self.epsilon)\n",
    "        else:\n",
    "            # Use moving statistics for inference\n",
    "            x_normalized = (x - self.moving_mean.reshape(1, 1, 1, -1)) / np.sqrt(self.moving_variance.reshape(1, 1, 1, -1) + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return x_normalized * self.gamma.reshape(1, 1, 1, -1) + self.beta.reshape(1, 1, 1, -1)\n",
    "\n",
    "class ManualBaseModel:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.built = False\n",
    "        \n",
    "    def build(self):\n",
    "        if self.built:\n",
    "            return\n",
    "            \n",
    "        # Better architecture with batch normalization\n",
    "        self.conv1 = ManualConv2D(32, 3, strides=2, padding='same')\n",
    "        self.bn1 = ManualBatchNormalization()\n",
    "        self.conv2 = ManualConv2D(64, 3, strides=2, padding='same')\n",
    "        self.bn2 = ManualBatchNormalization()\n",
    "        self.conv3 = ManualConv2D(128, 3, strides=2, padding='same')\n",
    "        self.bn3 = ManualBatchNormalization()\n",
    "        self.conv4 = ManualConv2D(256, 3, strides=2, padding='same')\n",
    "        self.bn4 = ManualBatchNormalization()\n",
    "        \n",
    "        self.layers = [\n",
    "            self.conv1, self.bn1,\n",
    "            self.conv2, self.bn2, \n",
    "            self.conv3, self.bn3,\n",
    "            self.conv4, self.bn4\n",
    "        ]\n",
    "        \n",
    "        # Initialize layers\n",
    "        current_shape = (1, 224, 224, 3)\n",
    "        for layer in self.layers:\n",
    "            current_shape = layer.initialize(current_shape)\n",
    "            \n",
    "        self.built = True\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        if not self.built:\n",
    "            self.build()\n",
    "            \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, ManualBatchNormalization):\n",
    "                x = layer.forward(x, training)\n",
    "                # ReLU activation after BN\n",
    "                if i < len(self.layers) - 1:  # Don't apply ReLU after last layer\n",
    "                    x = np.maximum(0, x)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class ManualEfficientNetB4:\n",
    "    def __init__(self, include_top=True, input_shape=(224, 224, 3), num_classes=5):\n",
    "        self.include_top = include_top\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.base_model = None\n",
    "        self.classifier_layers = []\n",
    "        self.built = False\n",
    "        \n",
    "    def build(self):\n",
    "        if self.built:\n",
    "            return\n",
    "            \n",
    "        # Use improved base model\n",
    "        self.base_model = ManualBaseModel()\n",
    "        self.base_model.build()\n",
    "        \n",
    "        if self.include_top:\n",
    "            self.global_pool = ManualGlobalAveragePooling2D()\n",
    "            self.dropout1 = ManualDropout(0.2)  # Reduced dropout\n",
    "            self.dense1 = ManualDense(512, activation='relu')  # More units\n",
    "            self.dropout2 = ManualDropout(0.2)  # Reduced dropout\n",
    "            self.dense2 = ManualDense(self.num_classes, activation='softmax')\n",
    "            \n",
    "            self.classifier_layers = [\n",
    "                self.global_pool,\n",
    "                self.dropout1,\n",
    "                self.dense1,\n",
    "                self.dropout2,\n",
    "                self.dense2\n",
    "            ]\n",
    "            \n",
    "            # Initialize classifier layers\n",
    "            base_output_shape = (1, 14, 14, 256)  # Updated output shape\n",
    "            current_shape = base_output_shape\n",
    "            \n",
    "            for layer in self.classifier_layers:\n",
    "                current_shape = layer.initialize(current_shape)\n",
    "            \n",
    "        self.built = True\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        if not self.built:\n",
    "            self.build()\n",
    "        \n",
    "        # Base model forward pass\n",
    "        x = self.base_model.forward(x, training)\n",
    "        \n",
    "        if self.include_top:\n",
    "            # Classifier forward pass\n",
    "            for layer in self.classifier_layers:\n",
    "                if isinstance(layer, ManualDropout):\n",
    "                    x = layer.forward(x, training)\n",
    "                else:\n",
    "                    x = layer.forward(x)\n",
    "                \n",
    "        return x\n",
    "\n",
    "def categorical_crossentropy(y_true, y_pred):\n",
    "    # More stable implementation\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1.0 - 1e-12)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Improved training with gradient descent\n",
    "class ManualSGD:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocities = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.velocities is None:\n",
    "            self.velocities = [np.zeros_like(p) for p in params]\n",
    "            \n",
    "        updated_params = []\n",
    "        for i, (param, grad, velocity) in enumerate(zip(params, grads, self.velocities)):\n",
    "            # Momentum update\n",
    "            velocity = self.momentum * velocity - self.lr * grad\n",
    "            updated_param = param + velocity\n",
    "            updated_params.append(updated_param)\n",
    "            self.velocities[i] = velocity\n",
    "            \n",
    "        return updated_params\n",
    "\n",
    "# Manual training implementation\n",
    "def manual_fit(model, train_generator, valid_generator, epochs=10, verbose=1):\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'accuracy': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "    \n",
    "    # Track best model\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        \n",
    "        # Training phase\n",
    "        train_generator.reset()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_generator:\n",
    "            # Forward pass\n",
    "            predictions = model.forward(batch_x, training=True)\n",
    "            \n",
    "            # Compute metrics\n",
    "            batch_loss = categorical_crossentropy(batch_y, predictions)\n",
    "            batch_accuracy = categorical_accuracy(batch_y, predictions)\n",
    "            \n",
    "            epoch_loss += batch_loss\n",
    "            epoch_accuracy += batch_accuracy\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Calculate averages\n",
    "        if num_batches > 0:\n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            avg_accuracy = epoch_accuracy / num_batches\n",
    "            history['loss'].append(avg_loss)\n",
    "            history['accuracy'].append(avg_accuracy)\n",
    "        \n",
    "        # Validation phase\n",
    "        valid_generator.reset()\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0.0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        for batch_x, batch_y in valid_generator:\n",
    "            predictions = model.forward(batch_x, training=False)\n",
    "            \n",
    "            batch_val_loss = categorical_crossentropy(batch_y, predictions)\n",
    "            batch_val_accuracy = categorical_accuracy(batch_y, predictions)\n",
    "            \n",
    "            val_loss += batch_val_loss\n",
    "            val_accuracy += batch_val_accuracy\n",
    "            num_val_batches += 1\n",
    "        \n",
    "        # Calculate validation averages\n",
    "        if num_val_batches > 0:\n",
    "            avg_val_loss = val_loss / num_val_batches\n",
    "            avg_val_accuracy = val_accuracy / num_val_batches\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_accuracy'].append(avg_val_accuracy)\n",
    "            \n",
    "            # Track best model\n",
    "            if avg_val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = avg_val_accuracy\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'loss: {avg_loss:.4f} - accuracy: {avg_accuracy:.4f} - '\n",
    "                  f'val_loss: {avg_val_loss:.4f} - val_accuracy: {avg_val_accuracy:.4f}')\n",
    "    \n",
    "    print(f\"Best validation accuracy: {best_val_accuracy * 100:.2f}%\")\n",
    "    return history\n",
    "\n",
    "def manual_evaluate(model, generator, verbose=1):\n",
    "    generator.reset()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_x, batch_y in generator:\n",
    "        predictions = model.forward(batch_x, training=False)\n",
    "        \n",
    "        batch_loss = categorical_crossentropy(batch_y, predictions)\n",
    "        batch_accuracy = categorical_accuracy(batch_y, predictions)\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        total_accuracy += batch_accuracy\n",
    "        num_batches += 1\n",
    "    \n",
    "    if num_batches > 0:\n",
    "        avg_loss = total_loss / num_batches\n",
    "        avg_accuracy = total_accuracy / num_batches\n",
    "    else:\n",
    "        avg_loss = avg_accuracy = 0.0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Evaluation - loss: {avg_loss:.4f} - accuracy: {avg_accuracy:.4f}')\n",
    "    \n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define dataset directories\n",
    "    base_dir = r\"E:\\marine-animals-dataset\\versions\\1\"\n",
    "    train_dir = os.path.join(base_dir, 'train')\n",
    "    valid_dir = os.path.join(base_dir, 'valid')\n",
    "    \n",
    "    # Create data generators\n",
    "    train_datagen = ManualImageDataGenerator(\n",
    "        target_size=(224, 224),\n",
    "        batch_size=16,  # Increased batch size\n",
    "        shuffle=True,\n",
    "        preprocessing_function=efficientnet_preprocess_input,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    \n",
    "    valid_datagen = ManualImageDataGenerator(\n",
    "        target_size=(224, 224),\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        preprocessing_function=efficientnet_preprocess_input\n",
    "    )\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading training data...\")\n",
    "    train_generator = train_datagen.flow_from_directory(train_dir, class_mode='categorical')\n",
    "    \n",
    "    print(\"Loading validation data...\")\n",
    "    valid_generator = valid_datagen.flow_from_directory(valid_dir, class_mode='categorical')\n",
    "    \n",
    "    # Create model\n",
    "    model = ManualEfficientNetB4(include_top=True, input_shape=(224, 224, 3), num_classes=5)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting manual training...\")\n",
    "    history = manual_fit(\n",
    "        model, \n",
    "        train_generator, \n",
    "        valid_generator, \n",
    "        epochs=10,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    val_loss, val_accuracy = manual_evaluate(model, valid_generator, verbose=1)\n",
    "    print(f\"Validation Accuracy after initial training: {val_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
